{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02 - Judge LLM Prompting: Corporate Credit Rating Report\n",
    "\n",
    "This notebook demonstrates how to develop, test, and iterate on prompts for the Credit Judge LLM, specifically focusing on generating a Corporate Credit Rating Report using a detailed template."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Prompt Template and Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import json # For potential JSON parsing of outputs\n",
    "# Assuming the notebook is run from the root of the 'credit_judge_poc' directory or the path is adjusted accordingly\n",
    "from credit_judge_poc.src.prompts.judge_prompts import CORPORATE_CREDIT_RATING_REPORT_TEMPLATE, SIMULATED_INPUT_REPORT_TEMPLATE, EXPERT_REVIEW_TABLE_GENERATION_TEMPLATE, COMPARISON_TABLE_GENERATION_TEMPLATE\n",
    "\n",
    "# Placeholder for LLM SDK - replace with actual SDK if used\n",
    "# import google.generativeai as genai # Example for Gemini API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define Input Variables for the Prompt\n",
    "\n",
    "These variables will be used to populate the prompt template. They include company details, optional financial overrides, special focus areas, and an optional human-written example report for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- User Inputs for the Report ---\n",
    "COMPANY_NAME = \"DraftKings Inc.\"\n",
    "TICKER_SYMBOL = \"DKNG\"\n",
    "STOCK_EXCHANGE = \"NASDAQ\" # Optional\n",
    "CURRENT_DATE = datetime.date.today().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# Optional: Override specific financial data\n",
    "# If None or \"N/A\", AI will use search. For N/A within overrides, AI will also use search for that specific item.\n",
    "SPECIFIC_FINANCIAL_PERIOD_OVERRIDE = \"Q1 2025\"\n",
    "SPECIFIC_REVENUE_OVERRIDE = \"$1,409 million (20% increase year-over-year)\"\n",
    "SPECIFIC_ADJ_EBITDA_OVERRIDE = \"$102.630 million\"\n",
    "SPECIFIC_NET_INCOME_OVERRIDE = \"N/A\" # Example: AI should search for this\n",
    "SPECIFIC_FCF_OVERRIDE = \"N/A\" # Example: AI should search for this\n",
    "SPECIFIC_OTHER_DATA_OVERRIDE = \"N/A\"\n",
    "\n",
    "SPECIAL_FOCUS_AREAS = \"Focus on the impact of the Jackpocket acquisition and path to sustained profitability.\" # Optional\n",
    "\n",
    "# Optional: Human example report for comparison\n",
    "HUMAN_EXAMPLE_REPORT_TEXT = \"\"\"\n",
    "I. Overview\n",
    "DraftKings Inc. operates in the dynamic and rapidly evolving digital sports entertainment and gaming industry. The company has demonstrated significant revenue and customer growth, but also faces challenges related to profitability and regulatory uncertainties. This report assesses DraftKings' creditworthiness, considering both quantitative and qualitative factors.\n",
    "\n",
    "II. Corporate Credit Rating\n",
    " * S&P Scale: BB-\n",
    " * Outlook: Stable\n",
    "Justification and Rationale:\n",
    " * The BB- rating reflects DraftKings' high growth and expanding market share, balanced against its current profitability profile and the inherent risks of the gaming industry.\n",
    " * DraftKings has successfully increased its revenue and customer base, indicating a strong competitive position.\n",
    " * However, the company's path to sustained profitability is still developing, and the industry is subject to regulatory changes and intense competition.\n",
    "Outlook:\n",
    " * The stable outlook indicates that DraftKings' credit rating is not expected to change significantly in the near term.\n",
    " * Continued revenue growth and progress towards profitability could lead to an upgrade, while failure to achieve these could result in a downgrade.\n",
    "III. Financial Performance\n",
    " * Revenue:\n",
    "   * First quarter 2025 revenue: $1,409 million (20% increase year-over-year)\n",
    "   * Revenue growth is driven by customer engagement, new customer acquisition, increased Sportsbook hold, and the acquisition of Jackpocket.\n",
    " * EBITDA:\n",
    "   * Adjusted EBITDA (first quarter 2025): $102.630 million\n",
    "   * EBITDA is improving, but the company is still working towards consistent profitability.\n",
    " * Free Cash Flow:\n",
    "   * Free cash flow is currently negative, as the company is investing heavily in growth and expansion.\n",
    "   * This is a key metric to monitor for improvement as the company matures.\n",
    " * Leverage:\n",
    "   * The company's leverage is moderate, with a mix of debt and equity financing.\n",
    "   * Debt levels are manageable but require monitoring in the context of ongoing investments and profitability.\n",
    "IV. Shared National Credit (SNC) Regulatory Rating\n",
    " * SNC Rating: Special Mention\n",
    " * Justification and Rationale:\n",
    "   * The Special Mention rating reflects potential weaknesses that may, if not corrected, affect the borrower's repayment capacity.\n",
    "   * While DraftKings exhibits strong growth potential, concerns about sustained profitability and cash flow warrant this classification.\n",
    " * Triggers:\n",
    "   * Triggers for a downgrade to a worse rating could include:\n",
    "     * Failure to achieve projected revenue growth\n",
    "     * Continued delays in achieving profitability\n",
    "     * Increased regulatory restrictions\n",
    "     * Significant increase in leverage\n",
    "   * Triggers for an upgrade could include:\n",
    "     * Consistent achievement of profitability and positive free cash flow\n",
    "     * Successful expansion into new markets\n",
    "     * Demonstrated ability to navigate regulatory challenges\n",
    " * Strengths:\n",
    "   * Strong brand recognition and market position\n",
    "   * High revenue growth and customer acquisition rates\n",
    "   * Expanding market access through new state legalization\n",
    "   * Technological innovation and product diversification\n",
    " * Weaknesses:\n",
    "   * Lack of consistent profitability\n",
    "   * Negative free cash flow\n",
    "   * Regulatory uncertainties and compliance costs\n",
    "   * Intense competition in the industry\n",
    "\"\"\" # Set to \"N/A\" or \"\" if no human example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Assemble the Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_prompt = CORPORATE_CREDIT_RATING_REPORT_TEMPLATE.format(\n",
    "    company_name=COMPANY_NAME,\n",
    "    ticker_symbol=TICKER_SYMBOL,\n",
    "    stock_exchange=STOCK_EXCHANGE if STOCK_EXCHANGE else \"N/A\",\n",
    "    current_date=CURRENT_DATE,\n",
    "    specific_financial_period_override=SPECIFIC_FINANCIAL_PERIOD_OVERRIDE if SPECIFIC_FINANCIAL_PERIOD_OVERRIDE else \"N/A\",\n",
    "    specific_revenue_override=SPECIFIC_REVENUE_OVERRIDE if SPECIFIC_REVENUE_OVERRIDE else \"N/A\",\n",
    "    specific_adj_ebitda_override=SPECIFIC_ADJ_EBITDA_OVERRIDE if SPECIFIC_ADJ_EBITDA_OVERRIDE else \"N/A\",\n",
    "    specific_net_income_override=SPECIFIC_NET_INCOME_OVERRIDE if SPECIFIC_NET_INCOME_OVERRIDE else \"N/A\",\n",
    "    specific_fcf_override=SPECIFIC_FCF_OVERRIDE if SPECIFIC_FCF_OVERRIDE else \"N/A\",\n",
    "    specific_other_data_override=SPECIFIC_OTHER_DATA_OVERRIDE if SPECIFIC_OTHER_DATA_OVERRIDE else \"N/A\",\n",
    "    special_focus_areas=SPECIAL_FOCUS_AREAS if SPECIAL_FOCUS_AREAS else \"N/A\",\n",
    "    human_example_report_text=HUMAN_EXAMPLE_REPORT_TEXT if HUMAN_EXAMPLE_REPORT_TEXT and HUMAN_EXAMPLE_REPORT_TEXT.strip() else \"N/A\"\n",
    ")\n",
    "\n",
    "print(\"--- COMPILED PROMPT FOR LLM ---\")\n",
    "print(final_prompt)\n",
    "print(\"--- END OF PROMPT ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. (Conceptual) Send Prompt to LLM and Get Response\n",
    "\n",
    "The following cell is a placeholder to illustrate where you would typically integrate with an LLM API (e.g., OpenAI, Anthropic, Google Gemini)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conceptual: Send to LLM API\n",
    "# \n",
    "# # Example using Google Gemini API (replace with your actual API key and setup)\n",
    "# try:\n",
    "#     genai.configure(api_key=\"YOUR_API_KEY\") \n",
    "#     model = genai.GenerativeModel(model_name=\"gemini-1.5-flash-latest\") # Or your preferred model\n",
    "# \n",
    "#     # Safety settings and generation config might be needed for optimal output\n",
    "#     safety_settings = [\n",
    "#         {\"category\": \"HARM_CATEGORY_HARASSMENT\", \"threshold\": \"BLOCK_NONE\"},\n",
    "#         {\"category\": \"HARM_CATEGORY_HATE_SPEECH\", \"threshold\": \"BLOCK_NONE\"},\n",
    "#         {\"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\", \"threshold\": \"BLOCK_NONE\"},\n",
    "#         {\"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\", \"threshold\": \"BLOCK_NONE\"},\n",
    "#     ]\n",
    "#     generation_config = {\n",
    "#         \"temperature\": 0.7, # Adjust for creativity/factuality\n",
    "#         \"top_p\": 1,\n",
    "#         \"top_k\": 1,\n",
    "#         \"max_output_tokens\": 8192, # Or as needed for long reports\n",
    "#     }\n",
    "# \n",
    "#     response = model.generate_content(\n",
    "#         final_prompt,\n",
    "#         generation_config=generation_config,\n",
    "#         safety_settings=safety_settings\n",
    "#     )\n",
    "#     print(\"\\n--- LLM RESPONSE ---\")\n",
    "#     print(response.text)\n",
    "#     llm_generated_report = response.text # Store for further processing\n",
    "# except Exception as e:\n",
    "#     print(f\"\\nAn error occurred: {e}\")\n",
    "#     llm_generated_report = None\n",
    "\n",
    "llm_generated_report = \"This is a placeholder for the LLM's generated report. Replace with actual LLM call.\"\n",
    "print(llm_generated_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Advanced Logic: Pre-processing and Post-processing\n",
    "\n",
    "This section outlines where more sophisticated analytical logic can be embedded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Pre-processing of Input Data\n",
    "\n",
    "Before populating the prompt, you might want to:\n",
    "- Fetch financial data from external APIs or databases.\n",
    "- Calculate key financial ratios (e.g., Debt/EBITDA, Current Ratio) to be included as context or specific overrides.\n",
    "- Perform data validation and cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conceptual: Pre-processing logic\n",
    "# def fetch_financial_data(ticker_symbol):\n",
    "#     # Placeholder: Implement fetching logic (e.g., from Yahoo Finance, IEX Cloud, internal DB)\n",
    "#     print(f\"Fetching data for {ticker_symbol}...\")\n",
    "#     return {\"revenue_latest_q\": \"$1.5B\", \"ebitda_latest_q\": \"$150M\"}\n",
    "# \n",
    "# if SPECIFIC_REVENUE_OVERRIDE == \"N/A\":\n",
    "#     financials = fetch_financial_data(TICKER_SYMBOL)\n",
    "     # This is where you would update the SPECIFIC_... variables before formatting the prompt\n",
    "#     # For example: SPECIFIC_REVENUE_OVERRIDE = financials.get('revenue_latest_q')\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Post-processing of LLM Output\n",
    "\n",
    "After receiving the LLM's response:\n",
    "- Parse the generated report (e.g., if it's structured text, markdown, or if you ask for JSON output).\n",
    "- Extract key findings, scores, and justifications.\n",
    "- Compare the LLM's output against a 'gold standard' human review (see `01_data_preparation.ipynb` for ideas on gold standard structure).\n",
    "- Implement more advanced evaluation metrics (see `evaluation_poc/evaluate_judge_outputs.py`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conceptual: Post-processing logic\n",
    "# def parse_llm_report(report_text):\n",
    "#     # Placeholder: Implement parsing logic based on expected report structure\n",
    "#     print(\"Parsing LLM report...\")\n",
    "#     # Example: extract inferred S&P rating\n",
    "#     # import re\n",
    "#     # match = re.search(r\"S&P Equivalent Scale: ([A-Za-z\\+\\-]+)\", report_text)\n",
    "#     # inferred_rating = match.group(1) if match else \"Not found\"\n",
    "#     # return {\"inferred_rating\": inferred_rating}\n",
    "#     return {\"parsed_data\": \"example data\"}\n",
    "# \n",
    "# if llm_generated_report:\n",
    "#     processed_output = parse_llm_report(llm_generated_report)\n",
    "#     print(f\"Processed output: {processed_output}\")\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Integrating Financial Ratio Calculations\n",
    "\n",
    "Demonstrate how financial ratios could be calculated and potentially used to inform the LLM or evaluate its output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conceptual: Financial ratio calculation\n",
    "# def calculate_debt_to_ebitda(total_debt, ebitda):\n",
    "#     if ebitda == 0: return float('inf') # Avoid division by zero\n",
    "#     return total_debt / ebitda\n",
    "# \n",
    "# # Example usage (assuming you've fetched/defined these values)\n",
    "# # total_company_debt = 2000 # in millions\n",
    "# # company_ebitda = 500 # in millions\n",
    "# # leverage_ratio = calculate_debt_to_ebitda(total_company_debt, company_ebitda)\n",
    "# # print(f\"Calculated Debt/EBITDA: {leverage_ratio}\")\n",
    "# \n",
    "# # This ratio could be part of the input to the LLM (e.g. in 'Other Data')\n",
    "# # or used in post-processing to verify LLM's financial understanding.\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Generating Simulated Input Reports (RAG-style)\n",
    "\n",
    "This section demonstrates how to use the `SIMULATED_INPUT_REPORT_TEMPLATE` to generate text content that mimics a Retrieval Augmented Generation (RAG) output. This can be used to create varied source material for testing the main credit report generation prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inputs for Simulated Input Report Template\n",
    "SIM_COMPANY_NAME = \"ExampleTech Inc.\"\n",
    "SIM_TICKER_SYMBOL = \"EXTI\"\n",
    "SIM_INDUSTRY = \"Software and Cloud Services\"\n",
    "SIM_FOCUS_AREAS = \"Recent AI product launches and their market reception, competitive landscape in cloud AI.\"\n",
    "\n",
    "filled_simulated_input_prompt = SIMULATED_INPUT_REPORT_TEMPLATE.format(\n",
    "    company_name=SIM_COMPANY_NAME,\n",
    "    ticker_symbol=SIM_TICKER_SYMBOL,\n",
    "    industry=SIM_INDUSTRY,\n",
    "    focus_areas=SIM_FOCUS_AREAS\n",
    ")\n",
    "\n",
    "print(\"--- SIMULATED INPUT REPORT PROMPT ---\")\n",
    "print(filled_simulated_input_prompt)\n",
    "print(\"--- END OF SIMULATED INPUT REPORT PROMPT ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conceptual LLM call for Simulated Input Report\n",
    "# response_sim_input = model.generate_content(filled_simulated_input_prompt, ...)\n",
    "# print(\"\\n--- LLM RESPONSE (Simulated RAG Output) ---\")\n",
    "# print(response_sim_input.text)\n",
    "llm_simulated_rag_output = \"Placeholder for LLM-generated RAG style text based on the prompt above.\"\n",
    "print(llm_simulated_rag_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Generating Expert Review Tables (JSON Output)\n",
    "\n",
    "This section demonstrates using `EXPERT_REVIEW_TABLE_GENERATION_TEMPLATE` to have an LLM act as a credit analyst reviewing a given report and producing a structured JSON review table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inputs for Expert Review Table Template\n",
    "INPUT_REPORT_TEXT_FOR_REVIEW = \"\"\" # This would be an actual AI or human written report text\n",
    "**I. Overview:** ExampleCorp is a major player in the advanced widget industry. Recent performance shows growth.\n",
    "**II. Financials:** Revenue for FY2023 was $250M, up 15% YoY. Net Income was $25M.\n",
    "**III. Strengths:** Strong brand, innovative products.\n",
    "**IV. Weaknesses:** High reliance on a single supplier for SuperWidgets.\n",
    "**V. Recommendation:** We assign a B+ rating due to market position but note supplier risk.\n",
    "\"\"\"\n",
    "REVIEW_REPORT_ID = \"ExampleCorp-FY2023-Review-001\"\n",
    "REVIEW_DATE = datetime.date.today().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "filled_expert_review_prompt = EXPERT_REVIEW_TABLE_GENERATION_TEMPLATE.format(\n",
    "    input_credit_report_text=INPUT_REPORT_TEXT_FOR_REVIEW,\n",
    "    report_id_placeholder=REVIEW_REPORT_ID,\n",
    "    current_date=REVIEW_DATE\n",
    ")\n",
    "\n",
    "print(\"--- EXPERT REVIEW TABLE GENERATION PROMPT ---\")\n",
    "print(filled_expert_review_prompt)\n",
    "print(\"--- END OF EXPERT REVIEW TABLE GENERATION PROMPT ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conceptual LLM call for Expert Review Table (instructing JSON output is key)\n",
    "# meta_instruction_for_json = \"Your response must be a single, valid JSON object as specified in the prompt.\"\n",
    "# response_expert_review = model.generate_content(meta_instruction_for_json + filled_expert_review_prompt, ...)\n",
    "# print(\"\\n--- LLM RESPONSE (Expert Review JSON) ---\")\n",
    "# try:\n",
    "#     parsed_json_review = json.loads(response_expert_review.text)\n",
    "#     print(json.dumps(parsed_json_review, indent=2))\n",
    "# except json.JSONDecodeError:\n",
    "#     print(\"Failed to parse LLM response as JSON:\")\n",
    "#     print(response_expert_review.text)\n",
    "llm_expert_review_json_output = {\n",
    "  \"reviewedReportId\": REVIEW_REPORT_ID,\n",
    "  \"reviewerName\": \"AI Credit Review Expert\",\n",
    "  \"reviewDate\": REVIEW_DATE,\n",
    "  \"overallAssessment\": {\n",
    "    \"comments\": \"The report provides a decent high-level summary but lacks depth in financial analysis and risk quantification. The supplier risk is noted but not explored sufficiently.\",\n",
    "    \"overallScore\": 6.5,\n",
    "    \"ratingConcurrence\": \"Neutral\"\n",
    "  },\n",
    "  \"sectionReviews\": [\n",
    "    {\n",
    "      \"sectionName\": \"Overview\",\n",
    "      \"qualitativeFeedback\": \"Clear and concise overview.\",\n",
    "      \"quantitativeScore\": 8,\n",
    "      \"dataAccuracy\": \"N/A\",\n",
    "      \"completeness\": 7,\n",
    "      \"analyticalDepth\": 6\n",
    "    },\n",
    "    {\n",
    "      \"sectionName\": \"Recommendation\",\n",
    "      \"qualitativeFeedback\": \"Rating justification is brief. More linkage between strengths/weaknesses and the final rating would be beneficial.\",\n",
    "      \"quantitativeScore\": 6,\n",
    "      \"dataAccuracy\": \"N/A\",\n",
    "      \"completeness\": 6,\n",
    "      \"analyticalDepth\": 5\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "print(json.dumps(llm_expert_review_json_output, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Generating Comparison Tables (Markdown Output)\n",
    "\n",
    "This section shows how to use `COMPARISON_TABLE_GENERATION_TEMPLATE` to have an LLM generate a Markdown table comparing an AI-generated report against a human expert example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inputs for Comparison Table Template\n",
    "COMP_COMPANY_NAME = \"ExampleCorp\"\n",
    "AI_GENERATED_REPORT_TEXT_FOR_COMP = \"\"\" # Simplified AI report text\n",
    "**I. Overview:** ExampleCorp (AI): Operates globally. Strong Q1 performance.\n",
    "**II. Rating:** AI assigns A- rating. Outlook: Stable. Key driver: market expansion.\n",
    "\"\"\"\n",
    "HUMAN_EXPERT_REPORT_TEXT_FOR_COMP = \"\"\" # Simplified Human expert report text\n",
    "**I. Overview:** ExampleCorp (Human): Focused on North America. Q1 performance was modest due to competition.\n",
    "**II. Rating:** Human expert suggests BB+ rating. Outlook: Negative. Key driver: margin compression and competitive threats.\n",
    "\"\"\"\n",
    "\n",
    "filled_comparison_table_prompt = COMPARISON_TABLE_GENERATION_TEMPLATE.format(\n",
    "    company_name=COMP_COMPANY_NAME,\n",
    "    ai_generated_report_text=AI_GENERATED_REPORT_TEXT_FOR_COMP,\n",
    "    human_expert_report_text=HUMAN_EXPERT_REPORT_TEXT_FOR_COMP\n",
    ")\n",
    "\n",
    "print(\"--- COMPARISON TABLE GENERATION PROMPT ---\")\n",
    "print(filled_comparison_table_prompt)\n",
    "print(\"--- END OF COMPARISON TABLE GENERATION PROMPT ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conceptual LLM call for Comparison Table (instructing Markdown output)\n",
    "# meta_instruction_for_markdown = \"Your response must be a single, valid Markdown document as specified in the prompt.\"\n",
    "# response_comparison_table = model.generate_content(meta_instruction_for_markdown + filled_comparison_table_prompt, ...)\n",
    "# print(\"\\n--- LLM RESPONSE (Comparison Table Markdown) ---\")\n",
    "# print(response_comparison_table.text)\n",
    "llm_comparison_table_markdown_output = \"\"\"# Placeholder for LLM-generated Markdown\n",
    "| Section | AI Report Summary & Key Points | Human Expert Report Summary & Key Points | Alignment & Differences Noted (Data, Nuances, Omissions) |\n",
    "|---|---|---|---|\n",
    "| Overview | ExampleCorp (AI): Operates globally. Strong Q1 performance. | ExampleCorp (Human): Focused on North America. Q1 performance was modest due to competition. | Difference in geographic focus and Q1 performance assessment. |\n",
    "| Rating | AI assigns A- rating. Outlook: Stable. Key driver: market expansion. | Human expert suggests BB+ rating. Outlook: Negative. Key driver: margin compression and competitive threats. | Significant difference in rating and outlook. AI more optimistic, Human more cautious focusing on competition. |\n",
    "\n",
    "## Performance Score of AI Report (vs. Human Expert Example)\n",
    "*   **Adherence to Format & Structure:** 8/10\n",
    "    *   *Justification:* AI followed a basic structure.\n",
    "*   **Accuracy/Comparability of Data:** 5/10\n",
    "    *   *Justification:* AI's Q1 assessment differs significantly from Human expert, needs verification.\n",
    "*   **Overall Score:** 6.5/10\n",
    "\"\"\"\n",
    "print(llm_comparison_table_markdown_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
