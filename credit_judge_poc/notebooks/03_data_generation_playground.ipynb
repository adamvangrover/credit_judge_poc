{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03 - Data Generation Playground\n",
    "\n",
    "This notebook provides a workspace to use the defined prompt templates to generate various types of mock data for the Credit Judge PoC. You can use this to create:\n",
    "1.  **Simulated Input Reports (RAG-style):** Text snippets mimicking retrieved data for LLM context.\n",
    "2.  **AI-Generated Credit Reports (JSON):** Full credit reports generated by an LLM based on the detailed template.\n",
    "3.  **Expert Human-Style Review Tables (JSON):** Reviews of existing reports, generated by an LLM acting as an expert.\n",
    "4.  **Comparison Tables (Markdown):** Detailed comparisons between AI reports and human/expert reports.\n",
    "\n",
    "**Objective:** To easily generate and save varied datasets for developing, testing, and refining the PoC components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Assuming the notebook is run from the root of 'credit_judge_poc' or paths adjusted\n",
    "from credit_judge_poc.src.prompts.judge_prompts import (\n",
    "    CORPORATE_CREDIT_RATING_REPORT_TEMPLATE,\n",
    "    SIMULATED_INPUT_REPORT_TEMPLATE,\n",
    "    EXPERT_REVIEW_TABLE_GENERATION_TEMPLATE,\n",
    "    COMPARISON_TABLE_GENERATION_TEMPLATE\n",
    ")\n",
    "\n",
    "# Placeholder for LLM SDK - replace with actual SDK if used\n",
    "# import google.generativeai as genai\n",
    "# GENAI_API_KEY = os.getenv(\"YOUR_LLM_API_KEY_ENV_VAR\") # Example\n",
    "# if GENAI_API_KEY:\n",
    "#     genai.configure(api_key=GENAI_API_KEY)\n",
    "\n",
    "# --- Output Directory Setup ---\n",
    "# Create a general directory for generated mock data if it doesn't exist\n",
    "GENERATED_DATA_DIR = os.path.join(\"..\", \"data\", \"generated_mock_data\")\n",
    "SUBDIRS = {\n",
    "    \"simulated_inputs\": os.path.join(GENERATED_DATA_DIR, \"simulated_input_reports\"),\n",
    "    \"ai_reports\": os.path.join(GENERATED_DATA_DIR, \"ai_credit_reports\"),\n",
    "    \"expert_reviews\": os.path.join(GENERATED_DATA_DIR, \"expert_review_tables\"),\n",
    "    \"comparison_tables\": os.path.join(GENERATED_DATA_DIR, \"comparison_tables\")\n",
    "}\n",
    "\n",
    "for subdir in SUBDIRS.values():\n",
    "    os.makedirs(subdir, exist_ok=True)\n",
    "\n",
    "print(f\"Generated data will be saved under: {os.path.abspath(GENERATED_DATA_DIR)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Helper Function for Conceptual LLM Call & Saving Output\n",
    "\n",
    "This is a conceptual function. You'll need to integrate your actual LLM calling logic here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_content_with_llm(prompt_text, output_filename, output_subdir_key, output_type=\"text\"):\n",
    "    \"\"\"\n",
    "    Conceptual function to simulate an LLM call and save the output.\n",
    "    Replace with your actual LLM interaction logic.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Sending Prompt to LLM (Conceptual) ---\")\n",
    "    # print(prompt_text[:500] + \"...\") # Print snippet of prompt\n",
    "    \n",
    "    # --- !!! REPLACE THIS WITH ACTUAL LLM CALL !!! ---\n",
    "    # model = genai.GenerativeModel(model_name=\"gemini-1.5-flash-latest\")\n",
    "    # response = model.generate_content(prompt_text)\n",
    "    # llm_output = response.text\n",
    "    llm_output = f\"Placeholder LLM output for: {output_filename}. Content based on prompt type: {output_type}.\"\n",
    "    if output_type == \"json\":\n",
    "        llm_output = json.dumps({\"placeholder_data\": \"This is JSON output for \" + output_filename, \"prompt_used\": prompt_text[:100] + \"...\"}, indent=2)\n",
    "    elif output_type == \"markdown\":\n",
    "        llm_output = f\"# {output_filename}\\n\\nThis is Markdown placeholder content.\"\n",
    "    # --- END OF REPLACE SECTION ---\n",
    "    \n",
    "    print(f\"--- LLM Response Received (Conceptual) ---\")\n",
    "    # print(llm_output[:500] + \"...\")\n",
    "\n",
    "    # Save the output\n",
    "    filepath = os.path.join(SUBDIRS[output_subdir_key], output_filename)\n",
    "    try:\n",
    "        with open(filepath, 'w', encoding='utf-8') as f:\n",
    "            f.write(llm_output)\n",
    "        print(f\"Successfully saved output to: {filepath}\")\n",
    "    except IOError as e:\n",
    "        print(f\"Error saving file {filepath}: {e}\")\n",
    "    \n",
    "    return llm_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Generate Simulated Input Reports (RAG-style Text)\n",
    "\n",
    "Use `SIMULATED_INPUT_REPORT_TEMPLATE`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_input_scenarios = [\n",
    "    {\"company_name\": \"FutureGadgets Corp.\", \"ticker_symbol\": \"FGCP\", \"industry\": \"Consumer Electronics\", \"focus_areas\": \"New VR headset launch, supply chain issues\"},\n",
    "    {\"company_name\": \"GreenEnergy Solutions\", \"ticker_symbol\": \"GES\", \"industry\": \"Renewable Energy\", \"focus_areas\": \"Impact of new government subsidies, competitor innovations\"}\n",
    "]\n",
    "\n",
    "for i, scenario in enumerate(sim_input_scenarios):\n",
    "    prompt = SIMULATED_INPUT_REPORT_TEMPLATE.format(**scenario)\n",
    "    filename = f\"{scenario['ticker_symbol']}_simulated_input_{i+1}.txt\"\n",
    "    # generate_content_with_llm(prompt, filename, \"simulated_inputs\", output_type=\"text\") # Uncomment to run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Generate AI Credit Reports (JSON Output)\n",
    "\n",
    "Use `CORPORATE_CREDIT_RATING_REPORT_TEMPLATE`. Remember to instruct the LLM (via meta-prompt if needed) to produce JSON output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ai_report_scenarios = [\n",
    "    {\n",
    "        \"company_name\": \"FutureGadgets Corp.\", \"ticker_symbol\": \"FGCP\", \"stock_exchange\": \"NASDAQ\", \n",
    "        \"current_date\": \"2024-05-15\", \"specific_financial_period_override\": \"N/A\",\n",
    "        \"specific_revenue_override\": \"N/A\", \"specific_adj_ebitda_override\": \"N/A\",\n",
    "        \"specific_net_income_override\": \"N/A\", \"specific_fcf_override\": \"N/A\",\n",
    "        \"specific_other_data_override\": \"N/A\", \"special_focus_areas\": \"VR market penetration strategy\",\n",
    "        \"human_example_report_text\": \"N/A\" # Or provide one to test comparison generation within this prompt\n",
    "    }\n",
    "]\n",
    "\n",
    "for i, scenario in enumerate(ai_report_scenarios):\n",
    "    prompt = CORPORATE_CREDIT_RATING_REPORT_TEMPLATE.format(**scenario)\n",
    "    # Crucial: Add meta-instruction for LLM to output JSON for this specific prompt.\n",
    "    meta_instruction_for_json = \"Your response must be a single, valid JSON object representing the credit report, based on the structure implied by the input template. \"\n",
    "    full_prompt_for_json_ai_report = meta_instruction_for_json + \"\\n\\n--- TEMPLATE START ---\\n\" + prompt + \"\\n--- TEMPLATE END ---\"\n",
    "    filename = f\"{scenario['ticker_symbol']}_ai_credit_report_{i+1}.json\"\n",
    "    # generate_content_with_llm(full_prompt_for_json_ai_report, filename, \"ai_reports\", output_type=\"json\") # Uncomment to run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Generate Expert Human-Style Review Tables (JSON Output)\n",
    "\n",
    "Use `EXPERT_REVIEW_TABLE_GENERATION_TEMPLATE`. This requires an existing report text as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume we have a generated AI report text (or human report text) to review\n",
    "# For example, load one of the AI reports generated in the previous step or use a placeholder.\n",
    "report_to_review_filename = os.path.join(SUBDIRS[\"ai_reports\"], \"FGCP_ai_credit_report_1.json\") # Example path\n",
    "input_report_text = \"Placeholder: Text of FGCP_ai_credit_report_1.json would go here. Or load from file.\"\n",
    "try:\n",
    "    with open(report_to_review_filename, 'r') as f:\n",
    "        loaded_report_json = json.load(f)\n",
    "        input_report_text = json.dumps(loaded_report_json, indent=2) # Or a summarized text version\n",
    "except FileNotFoundError:\n",
    "    print(f\"File not found: {report_to_review_filename}, using placeholder text for review generation.\")\n",
    "\n",
    "expert_review_scenarios = [\n",
    "    {\"report_id_placeholder\": \"FGCP_Report_Review_1\", \"current_date\": \"2024-05-16\", \"input_credit_report_text\": input_report_text}\n",
    "]\n",
    "\n",
    "for i, scenario in enumerate(expert_review_scenarios):\n",
    "    prompt = EXPERT_REVIEW_TABLE_GENERATION_TEMPLATE.format(**scenario)\n",
    "    filename = f\"{scenario['report_id_placeholder']}_expert_review_{i+1}.json\"\n",
    "    # generate_content_with_llm(prompt, filename, \"expert_reviews\", output_type=\"json\") # Uncomment to run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Generate Comparison Tables (Markdown Output)\n",
    "\n",
    "Use `COMPARISON_TABLE_GENERATION_TEMPLATE`. Requires two report texts: an AI one and a Human/Expert one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume we have an AI report and a Human Expert report text\n",
    "ai_report_for_comp_text = \"Placeholder: Text of FGCP_ai_credit_report_1.json...\"\n",
    "# You might load an actual generated AI report here\n",
    "try:\n",
    "    with open(os.path.join(SUBDIRS[\"ai_reports\"], \"FGCP_ai_credit_report_1.json\"), 'r') as f:\n",
    "        ai_report_for_comp_text = json.dumps(json.load(f), indent=2) # Or a summarized text version\n",
    "except FileNotFoundError:\n",
    "    print(\"AI report for comparison not found, using placeholder.\")\n",
    "\n",
    "human_expert_report_for_comp_text = \"\"\" # Placeholder for a human expert report on the same company\n",
    "**I. Overview (Human Expert for FGCP):** FutureGadgets Corp. shows promise but faces significant market headwinds. VR adoption is slower than anticipated.\n",
    "**II. Financials (Human Expert):** Revenue growth is primarily from legacy products, VR sales are lagging. Margins are under pressure.\n",
    "**III. Rating (Human Expert):** Assign B rating, Outlook Negative, due to execution risk in VR and competitive threats.\n",
    "\"\"\"\n",
    "\n",
    "comparison_scenarios = [\n",
    "    {\n",
    "        \"company_name\": \"FutureGadgets Corp.\", \n",
    "        \"ai_generated_report_text\": ai_report_for_comp_text,\n",
    "        \"human_expert_report_text\": human_expert_report_for_comp_text\n",
    "    }\n",
    "]\n",
    "\n",
    "for i, scenario in enumerate(comparison_scenarios):\n",
    "    prompt = COMPARISON_TABLE_GENERATION_TEMPLATE.format(**scenario)\n",
    "    filename = f\"{scenario['company_name'].replace(' ', '')}_comparison_table_{i+1}.md\"\n",
    "    # generate_content_with_llm(prompt, filename, \"comparison_tables\", output_type=\"markdown\") # Uncomment to run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1.  Replace the conceptual `generate_content_with_llm` function with actual calls to your chosen LLM API.\n",
    "2.  Customize the scenarios and input data to generate a diverse dataset.\n",
    "3.  Use the generated data to test parsing, formatting, and evaluation components of the PoC.\n",
    "4.  Iterate on the prompts themselves based on the quality of the LLM outputs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12" 
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
