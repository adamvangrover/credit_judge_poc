# Cell 1: Notebook Title and Introduction (Markdown)
# ```markdown
# # Interactive Comparative Workflow for Simulated LLM and Expert Credit Reports
#
# This Jupyter Notebook provides an interactive framework to simulate, analyze, and compare credit reports. It's designed to be adaptable for different companies (defaulting to OpenAI) and allows for both simulated report generation and user-inputted report text.
#
# **Key Features:**
# * Interactive controls to adjust simulation parameters.
# * Dynamic generation of simulated LLM and Expert report snippets.
# * Option to input custom report text.
# * A comparative table and chart that update based on inputs and simulated scores.
# * Simulated "expert take" and "counterargument" generation.
# ```

# Cell 2: Essential Library Imports (Code)
# ```python
# Core data manipulation
import pandas as pd
import numpy as np

# NLP libraries (ensure you have them installed and spacy model downloaded)
import spacy
from textblob import TextBlob
import textstat
# Example: !python -m spacy download en_core_web_sm
# nlp = spacy.load('en_core_web_sm') # Load spacy model if doing actual NLP

# Visualization
import matplotlib.pyplot as plt
import seaborn as sns

# Utilities
import re
import random
import json # For deep copying scores

# IPython and ipywidgets for interactivity
from IPython.display import display, HTML, clear_output
import ipywidgets as widgets
from ipywidgets import HBox, VBox, Layout, interactive_output

# Apply a seaborn style for nicer plots
sns.set_theme(style="whitegrid")

print("Libraries imported successfully.")
# ```

# Cell 3: Company Configuration (Markdown)
# ```markdown
# ## Configure Company for Analysis
# Specify the company name you want to use for the simulation.
# ```

# Cell 4: Company Name Widget (Code)
# ```python
company_name_widget = widgets.Text(
    value='OpenAI',
    placeholder='Enter company name',
    description='Company:',
    disabled=False,
    layout=Layout(width='50%')
)
display(company_name_widget)

def get_company_name():
    return company_name_widget.value

print(f"Default company: {get_company_name()}")
# ```

# Cell 5: Foundational Setup & Helper Functions (Markdown)
# ```markdown
# ## II. Foundational Setup
#
# This section mirrors Part II of the provided HTML, focusing on notebook organization and helper functions.
#
# ### A. Best Practices for Jupyter Notebook Organization
# * **Modular Notebook Design & Logical Sectioning:** Use Markdown for headings and explanations. Keep code cells focused on specific tasks.
# * **Version Control (Git) and `requirements.txt`:** Recommended for larger projects.
#
# ### C. Defining Helper Functions for Common Tasks
# (Simulated text processing and scoring functions will be defined later)
# ```

# Cell 6: Helper Functions (Code)
# ```python
# Placeholder for text cleaning if actual NLP is performed
def clean_text(text):
    text = re.sub(r'\s+', ' ', text).strip()
    # Add more cleaning steps as needed
    return text

# --- NLP Scoring Functions (Placeholders/Examples) ---
# These would be used if analyzing user-inputted raw text.
# The current interactive table uses SIMULATED scores primarily.

def calculate_sentiment_textblob(text):
    if not text: return (0, 0)
    blob = TextBlob(text)
    return (blob.sentiment.polarity, blob.sentiment.subjectivity)

def calculate_readability_metrics(text):
    if not text: return {'flesch_reading_ease': 0, 'gunning_fog': 0}
    return {
        'flesch_reading_ease': textstat.flesch_reading_ease(text),
        'gunning_fog': textstat.gunning_fog(text)
    }

def count_specific_keywords(text, keyword_list):
    if not text: return 0
    count = 0
    words = text.lower().split()
    for keyword in keyword_list:
        count += words.count(keyword.lower())
    # For density (per 1k words):
    # num_words = len(words)
    # density = (count / num_words * 1000) if num_words > 0 else 0
    return count

# You can add more helper functions here (e.g., for saving/loading text)
print("Helper function structure defined.")
# ```

# Cell 7: Interactive Controls & Assumptions (Markdown)
# ```markdown
# ## 0. Interactive Controls & Assumptions (Simulated)
#
# Adjust the parameters below to simulate different scenarios for the selected company. Changes will dynamically update the simulated report snippets, the comparison table, and the chart.
# ```

# Cell 8: Interactive Control Widgets (Code)
# ```python
# Initial Slider Values (from HTML JavaScript)
initial_slider_values = {
    'revenueAssumption': 3.0,
    'rdSpendAssumption': 5.0,
    'marketCompLevel': 7,
    'regScrutinyLevel': 6,
    'llmOptimism': 7,
    'expertCriticality': 7,
}

style = {'description_width': 'initial'}
slider_layout = Layout(width='80%')

revenue_slider = widgets.FloatSlider(value=initial_slider_values['revenueAssumption'], min=1, max=20, step=0.5, description='Projected Annual Revenue (USD Billions):', readout_format='.1f', style=style, layout=slider_layout)
rd_spend_slider = widgets.FloatSlider(value=initial_slider_values['rdSpendAssumption'], min=1, max=15, step=0.5, description='Projected R&D Spend (USD Billions):', readout_format='.1f', style=style, layout=slider_layout)
market_comp_slider = widgets.IntSlider(value=initial_slider_values['marketCompLevel'], min=1, max=10, step=1, description='Market Competitiveness Level (1-Low, 10-High):', style=style, layout=slider_layout)
reg_scrutiny_slider = widgets.IntSlider(value=initial_slider_values['regScrutinyLevel'], min=1, max=10, step=1, description='Regulatory Scrutiny Level (1-Low, 10-High):', style=style, layout=slider_layout)
llm_optimism_slider = widgets.IntSlider(value=initial_slider_values['llmOptimism'], min=1, max=10, step=1, description='LLM Report Optimism (1-Pessimistic, 10-Very Optimistic):', style=style, layout=slider_layout)
expert_criticality_slider = widgets.IntSlider(value=initial_slider_values['expertCriticality'], min=1, max=10, step=1, description='Expert Report Criticality (1-Lenient, 10-Very Critical):', style=style, layout=slider_layout)

reset_button = widgets.Button(description="Reset to Defaults", button_style='warning')
controls_box = VBox([
    revenue_slider, rd_spend_slider, market_comp_slider,
    reg_scrutiny_slider, llm_optimism_slider, expert_criticality_slider,
    reset_button
])

display(controls_box)
print("Interactive control widgets created.")
# ```

# Cell 9: Data Structures for Scores and Report Content (Code)
# ```python
# Baseline Scores (from HTML JavaScript)
baseline_scores = {
    'qualitative': [
        {'name': 'Clarity & Coherence', 'llm': 4, 'expert': 5, 'editable': True},
        {'name': 'Depth of Analysis', 'llm': 2, 'expert': 4, 'editable': True},
        {'name': 'Identification & Articulation of Key Risks', 'llm': 3, 'expert': 5, 'editable': True},
        {'name': 'Actionability/Insightfulness', 'llm': 2, 'expert': 4, 'editable': True},
        {'name': 'Specificity to Company', 'llm': 3, 'expert': 5, 'editable': True},
        {'name': 'Balanced Perspective', 'llm': 4, 'expert': 4, 'editable': True}
    ],
    'quantitative': [
        {'name': 'Overall Sentiment: Polarity', 'llm': 0.15, 'expert': 0.05, 'type': 'float', 'editable': False},
        {'name': 'Overall Sentiment: Subjectivity', 'llm': 0.40, 'expert': 0.60, 'type': 'float', 'editable': False},
        {'name': 'Readability: Flesch Reading Ease', 'llm': 55.0, 'expert': 40.0, 'type': 'integer', 'editable': False}, # Ensure float for calc, format later
        {'name': 'Density: "Risk" keywords (per 1k words)', 'llm': 5.2, 'expert': 9.5, 'type': 'float', 'editable': False},
        {'name': 'Count: Numerical Figures', 'llm': 12.0, 'expert': 25.0, 'type': 'integer', 'editable': False}, # Ensure float for calc
        {'name': 'Avg. Sentence Length (words)', 'llm': 18.0, 'expert': 22.0, 'type': 'integer', 'editable': False} # Ensure float for calc
    ]
}

# Deep copy for current scores to allow modifications
current_scores_data = json.loads(json.dumps(baseline_scores))

# Dictionary to store user-edited qualitative scores
user_edited_qualitative_scores = {}

# Global dictionary to hold report texts
report_texts = {
    "llm_simulated": "",
    "expert_simulated": "",
    "llm_user_input": "",
    "expert_user_input": ""
}

print("Score data structures initialized.")
# ```

# Cell 10: Widgets for Qualitative Score Editing (Code)
# ```python
qualitative_score_widgets = {}
qualitative_score_inputs_vbox_list = [widgets.HTML("<h3>Edit Qualitative Scores (1-5):</h3>")]

for i, metric in enumerate(current_scores_data['qualitative']):
    llm_input = widgets.BoundedIntText(value=metric['llm'], min=1, max=5, step=1, layout=Layout(width='60px'))
    expert_input = widgets.BoundedIntText(value=metric['expert'], min=1, max=5, step=1, layout=Layout(width='60px'))

    qualitative_score_widgets[i] = {'llm_widget': llm_input, 'expert_widget': expert_input}

    hbox = HBox([
        widgets.Label(metric['name'], layout=Layout(width='300px')),
        widgets.Label("LLM:"), llm_input,
        widgets.Label("Expert:"), expert_input
    ])
    qualitative_score_inputs_vbox_list.append(hbox)

qualitative_scores_editor_vbox = VBox(qualitative_score_inputs_vbox_list)
# This will be displayed later, along with other outputs.
print("Widgets for qualitative score editing created.")
# ```

# Cell 11: Functions to Simulate Report Content (Code)
# ```python
# Translated from HTML JavaScript
def generate_llm_snippets(company, optimism, revenue, market_comp):
    global report_texts
    revenue_snippet, risk_snippet = "", ""
    if optimism <= 3:
        revenue_snippet = f"For {company}, projected revenues of ${revenue:.1f}B face significant headwinds. Market competition (currently {market_comp}/10) is intensifying, potentially impacting API pricing and adoption rates. Growth appears challenging."
        risk_snippet = "The evolving global regulatory framework poses substantial and immediate threats. Compliance challenges are mounting, and societal concerns could severely impact adoption and increase operational costs."
    elif optimism <= 7:
        revenue_snippet = f"{company} demonstrates steady revenue growth towards ${revenue:.1f}B, primarily driven by API services and enterprise solutions. However, with market competition at {market_comp}/10, future growth may moderate without new breakthroughs."
        risk_snippet = "The global AI regulatory framework presents uncertainties. Compliance with data privacy and addressing societal concerns are important areas of focus, requiring ongoing vigilance."
    else:
        revenue_snippet = f"{company} is experiencing explosive revenue growth, targeting ${revenue:.1f}B, fueled by groundbreaking API services and rapid expansion into new enterprise verticals. Despite market competition at {market_comp}/10, strong upward momentum is evident."
        risk_snippet = f"While the AI regulatory landscape is developing, {company} is proactively shaping standards and addressing societal concerns, positioning to turn potential risks into opportunities for industry leadership."

    full_report = f"LLM Simulated Report for {company}:\n\nFinancial Overview (Revenue):\n{revenue_snippet}\n\nRisk Factors (Regulatory):\n{risk_snippet}\n\nAdditional AI Generated Content: Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum. We project significant advancements in model capabilities and a broadening user base across diverse sectors. Synergies with strategic partners are expected to unlock new revenue streams. The competitive landscape necessitates continuous innovation and agile adaptation to maintain market leadership."
    report_texts["llm_simulated"] = full_report
    return f"<strong>Financial Overview (Revenue):</strong> {revenue_snippet}<br><strong>Risk Factors (Regulatory):</strong> {risk_snippet}"


def generate_expert_financial_analysis(company, revenue, rd_spend, criticality, market_comp, reg_scrutiny):
    global report_texts
    rd_to_revenue_ratio = (rd_spend / revenue) if revenue > 0 else float('inf')
    analysis = f"Expert Financial Analysis for {company} (May 2025 Outlook):\nProjected revenues: ${revenue:.1f}B | R&D Spend: ${rd_spend:.1f}B\nMarket Competitiveness: {market_comp}/10 | Regulatory Scrutiny: {reg_scrutiny}/10\n\n"

    if rd_to_revenue_ratio > 1.2:
        analysis += f"The aggressive R&D spend, at {((rd_to_revenue_ratio) * 100):.0f}% of projected revenue, significantly outpaces direct revenue conversion. "
        if criticality > 7:
            analysis += "This strategy, while targeting technological dominance, presents a material near-term profitability challenge and heightens reliance on external funding. Cash burn rates and funding runway require stringent monitoring, especially with current market dynamics. "
        elif criticality > 4:
            analysis += "This indicates a continued intensive investment phase. While common for growth-focused tech aiming for breakthroughs, it introduces financial pressure that warrants careful observation of capital efficiency. "
        else:
            analysis += "This level of investment is bold, potentially securing long-term advantages, provided funding remains robust and accessible. The market will watch for monetization milestones. "
    elif revenue > 0:
        analysis += f"R&D spending is substantial but more aligned with current revenue streams ({((rd_to_revenue_ratio) * 100):.0f}% of revenue). "
        if criticality > 6:
            analysis += "Focus must remain on ensuring these investments translate to sustainable margin improvements and defendable market share, particularly as compute costs remain high and competition intensifies. "
        else:
            analysis += "This balanced approach allows for sustained innovation while managing financial exposure, a prudent strategy in a dynamic sector. "
    else: # No revenue
        analysis += f"With ${rd_spend:.1f}B in R&D and negligible current revenue, {company} operates as a pure research and development entity. Its valuation hinges entirely on future monetization potential and technological breakthroughs. This is a high-risk, high-reward profile. "

    analysis += f"\nGross margins will be a key watchpoint, given escalating compute requirements. Navigating external pressures from competition ({market_comp}/10) and regulation ({reg_scrutiny}/10) will be critical for achieving sustained profitability. A detailed SWOT analysis would be beneficial here, focusing on intellectual property, talent retention, and go-to-market strategy for nascent products. The path to positive cash flow requires clear milestones and disciplined execution."
    report_texts["expert_simulated"] = analysis
    return analysis.replace('\n', '<br>')

print("Report content generation functions defined.")
# ```

# Cell 12: Option for User to Input Reports (Markdown)
# ```markdown
# ## III. User Input: Provide Your Own Reports (Optional)
#
# If you have specific LLM-generated or Expert reports, you can paste their content into the text areas below. If provided, these will be used in the comparison. Otherwise, the system will use the simulated reports generated based on the interactive controls above.
# ```

# Cell 13: User Input Widgets (Code)
# ```python
llm_report_input_widget = widgets.Textarea(
    value='',
    placeholder=f'Paste LLM-generated report text for {get_company_name()} here...',
    description='LLM Report:',
    layout={'height': '150px', 'width': '90%'}
)

expert_report_input_widget = widgets.Textarea(
    value='',
    placeholder=f'Paste Expert-analyzed report text for {get_company_name()} here...',
    description='Expert Report:',
    layout={'height': '150px', 'width': '90%'}
)

report_input_box = VBox([
    widgets.HTML(f"<h3>Input Custom Reports for {get_company_name()} (Optional)</h3>"),
    llm_report_input_widget,
    expert_report_input_widget
])
display(report_input_box)

def update_user_report_texts():
    global report_texts
    report_texts["llm_user_input"] = llm_report_input_widget.value
    report_texts["expert_user_input"] = expert_report_input_widget.value
    # Update placeholders if company name changes and user hasn't typed
    if not llm_report_input_widget.value:
         llm_report_input_widget.placeholder = f'Paste LLM-generated report text for {get_company_name()} here...'
    if not expert_report_input_widget.value:
        expert_report_input_widget.placeholder = f'Paste Expert-analyzed report text for {get_company_name()} here...'

llm_report_input_widget.observe(lambda change: update_user_report_texts(), names='value')
expert_report_input_widget.observe(lambda change: update_user_report_texts(), names='value')
company_name_widget.observe(lambda change: update_user_report_texts(), names='value')


print("User report input widgets created.")
# ```

# Cell 14: Report Snippet Display Areas (Code)
# ```python
# --- Output areas for dynamic content ---
llm_snippets_display = widgets.HTML(value="LLM report snippets will appear here.")
expert_financial_analysis_snippet_display = widgets.HTML(value="Expert analysis snippets will appear here.")

reports_display_box = VBox([
    widgets.HTML("<h3>Simulated Report Snippets:</h3>"),
    HBox([
        VBox([widgets.HTML("<strong>LLM Simulated Snippets:</strong>"), llm_snippets_display], layout=Layout(width='50%', border='1px solid lightgray', padding='10px')),
        VBox([widgets.HTML("<strong>Expert Simulated Snippets:</strong>"), expert_financial_analysis_snippet_display], layout=Layout(width='50%', border='1px solid lightgray', padding='10px'))
    ])
])
# This will be displayed later within a larger output structure
print("Report snippet display areas created.")
# ```

# Cell 15: Core Logic: Recalculate Scores, Table, Chart (Code)
# ```python
# --- Global output widgets ---
# Comparison Table Display
comparison_table_output = widgets.Output()
# Comparison Chart Display
comparison_chart_output = widgets.Output()
# Main Output Area
main_output = widgets.Output() # For combined display

# --- Function to update qualitative scores from widgets ---
def update_qualitative_scores_from_widgets(change=None):
    global current_scores_data, user_edited_qualitative_scores
    for i, metric_widgets in qualitative_score_widgets.items():
        current_scores_data['qualitative'][i]['llm'] = metric_widgets['llm_widget'].value
        current_scores_data['qualitative'][i]['expert'] = metric_widgets['expert_widget'].value
        user_edited_qualitative_scores[i] = { # Track user edits
            'llm': metric_widgets['llm_widget'].value,
            'expert': metric_widgets['expert_widget'].value
        }
    # After manual edit, trigger a full recalculation and render
    # This ensures sliders don't overwrite manual edits immediately unless reset
    recalculate_and_render_all()


# Attach observers to qualitative score input widgets
for i in qualitative_score_widgets:
    qualitative_score_widgets[i]['llm_widget'].observe(update_qualitative_scores_from_widgets, names='value')
    qualitative_score_widgets[i]['expert_widget'].observe(update_qualitative_scores_from_widgets, names='value')


# --- Main recalculation and rendering function ---
def recalculate_and_render_all(change=None):
    global current_scores_data, baseline_scores, report_texts

    # Get current slider values
    company = get_company_name()
    revenue = revenue_slider.value
    rd_spend = rd_spend_slider.value
    market_comp = market_comp_slider.value
    reg_scrutiny = reg_scrutiny_slider.value
    llm_optimism = llm_optimism_slider.value
    expert_criticality = expert_criticality_slider.value
    rd_to_revenue_ratio = (rd_spend / revenue) if revenue > 0 else 10.0 # Handle revenue = 0

    # Deep copy baseline scores to current_scores for fresh calculation
    current_scores_data = json.loads(json.dumps(baseline_scores))

    # Re-apply user-edited qualitative scores if they exist
    for index, scores in user_edited_qualitative_scores.items():
        if index < len(current_scores_data['qualitative']):
            current_scores_data['qualitative'][index]['llm'] = scores['llm']
            current_scores_data['qualitative'][index]['expert'] = scores['expert']
            # Also update the widgets to reflect this persisted user edit
            qualitative_score_widgets[index]['llm_widget'].value = scores['llm']
            qualitative_score_widgets[index]['expert_widget'].value = scores['expert']


    # --- Apply simulated impacts to QUANTITATIVE scores ---
    # (Logic from HTML, slightly adapted for Python)
    # LLM Report Adjustments
    current_scores_data['quantitative'][0]['llm'] = np.clip(baseline_scores['quantitative'][0]['llm'] + (llm_optimism - 7) * 0.05 - (market_comp - 7) * 0.01, -1, 1) # Polarity
    current_scores_data['quantitative'][2]['llm'] = np.clip(baseline_scores['quantitative'][2]['llm'] + (llm_optimism - 7) * 2.5 - (5 if rd_to_revenue_ratio > 1.5 else 0), 0, 100) # Readability
    current_scores_data['quantitative'][4]['llm'] = np.maximum(0, baseline_scores['quantitative'][4]['llm'] - ( (llm_optimism - 8) * 2 if llm_optimism > 8 else 0) + (3 if revenue > 10 else 0) )# Num Figures

    # Expert Report Adjustments
    current_scores_data['quantitative'][0]['expert'] = np.clip(baseline_scores['quantitative'][0]['expert'] - (expert_criticality - 7) * 0.04 - (0.05 if revenue < 2 else 0), -1, 1) # Polarity
    current_scores_data['quantitative'][1]['expert'] = np.clip(baseline_scores['quantitative'][1]['expert'] + (expert_criticality - 7) * 0.03 + (reg_scrutiny - 6) * 0.02, 0, 1) # Subjectivity
    current_scores_data['quantitative'][3]['expert'] = np.maximum(0, baseline_scores['quantitative'][3]['expert'] + (reg_scrutiny - 6) * 0.5 + (market_comp - 7) * 0.3 + (expert_criticality - 7) * 0.4 + (2 if rd_to_revenue_ratio > 1.5 else 0) ) # Risk Density
    current_scores_data['quantitative'][4]['expert'] = np.maximum(0, baseline_scores['quantitative'][4]['expert'] + (revenue - 3) * 1.5 + (rd_spend - 5) * 0.8 + (3 if expert_criticality > 7 else 0)) # Num Figures
    current_scores_data['quantitative'][5]['expert'] = np.maximum(10, baseline_scores['quantitative'][5]['expert'] + (expert_criticality - 7) * 0.5 + (1 if reg_scrutiny > 7 else 0)) # Avg Sentence Length

    # --- Update Dynamic Report Snippets ---
    llm_snippets_html = generate_llm_snippets(company, llm_optimism, revenue, market_comp)
    llm_snippets_display.value = llm_snippets_html

    expert_analysis_html = generate_expert_financial_analysis(company, revenue, rd_spend, expert_criticality, market_comp, reg_scrutiny)
    expert_financial_analysis_snippet_display.value = expert_analysis_html

    # --- Populate Table and Update Chart ---
    populate_comparison_table()
    update_comparison_chart()

    # Update user report text area placeholders if company changes
    update_user_report_texts()


def populate_comparison_table():
    with comparison_table_output:
        clear_output(wait=True)
        table_data = []

        # Qualitative Scores
        table_data.append({'Criterion/Metric': '**Qualitative Assessment (1-5 Scale)**', 'LLM Report': '', 'Expert Report': '', 'Difference': ''})
        for i, metric in enumerate(current_scores_data['qualitative']):
            llm_score = qualitative_score_widgets[i]['llm_widget'].value # Use widget value directly
            expert_score = qualitative_score_widgets[i]['expert_widget'].value # Use widget value directly
            diff = llm_score - expert_score
            table_data.append({
                'Criterion/Metric': metric['name'],
                'LLM Report': llm_score,
                'Expert Report': expert_score,
                'Difference': f"{diff:.0f}"
            })

        # Quantitative Metrics
        table_data.append({'Criterion/Metric': '**Quantitative Metrics (Simulated)**', 'LLM Report': '', 'Expert Report': '', 'Difference': ''})
        for metric in current_scores_data['quantitative']:
            llm_val = metric['llm']
            exp_val = metric['expert']
            diff_val = llm_val - exp_val

            if metric['type'] == 'float':
                llm_f, exp_f, diff_f = f"{llm_val:.2f}", f"{exp_val:.2f}", f"{diff_val:.2f}"
            else: # integer
                llm_f, exp_f, diff_f = f"{llm_val:.0f}", f"{exp_val:.0f}", f"{diff_val:.0f}"

            table_data.append({
                'Criterion/Metric': metric['name'],
                'LLM Report': llm_f,
                'Expert Report': exp_f,
                'Difference': diff_f
            })

        df = pd.DataFrame(table_data)
        display(HTML(df.to_html(index=False, escape=False, classes='table table-striped table-bordered')))

def update_comparison_chart():
    with comparison_chart_output:
        clear_output(wait=True)

        qual_metrics = current_scores_data['qualitative']
        quant_metrics = current_scores_data['quantitative']

        # Use current values from widgets for qualitative, and calculated for quantitative
        llm_scores = [qualitative_score_widgets[i]['llm_widget'].value for i in range(len(qual_metrics))] + \
                     [m['llm'] for m in quant_metrics]
        expert_scores = [qualitative_score_widgets[i]['expert_widget'].value for i in range(len(qual_metrics))] + \
                        [m['expert'] for m in quant_metrics]

        labels_raw = [m['name'] for m in qual_metrics] + [m['name'] for m in quant_metrics]

        # Shorten labels for chart
        labels = [
            l.replace('Actionability/Insightfulness', 'Actionability')
             .replace('Identification & Articulation of Key Risks', 'Risk ID')
             .replace('Overall Sentiment: Polarity', 'Polarity')
             .replace('Overall Sentiment: Subjectivity', 'Subjectivity')
             .replace('Readability: Flesch Reading Ease', 'Readability')
             .replace('Density: "Risk" keywords (per 1k words)', 'Risk KWs')
             .replace('Count: Numerical Figures', 'Num. Figs')
             .replace('Avg. Sentence Length (words)', 'Sentence Len.')
            for l in labels_raw
        ]

        x = np.arange(len(labels))  # the label locations
        width = 0.35  # the width of the bars

        fig, ax = plt.subplots(figsize=(16, 7)) # Increased figure size
        rects1 = ax.bar(x - width/2, llm_scores, width, label='LLM Simulated Report', color='mediumpurple')
        rects2 = ax.bar(x + width/2, expert_scores, width, label='Expert Simulated Report', color='mediumseagreen')

        ax.set_ylabel('Scores / Values')
        ax.set_title(f'Comparative Analysis for {get_company_name()}', fontsize=16)
        ax.set_xticks(x)
        ax.set_xticklabels(labels, rotation=45, ha="right", fontsize=9) # Adjusted rotation and fontsize
        ax.legend()

        ax.bar_label(rects1, padding=3, fmt='%.1f', fontsize=8) # Show values on bars
        ax.bar_label(rects2, padding=3, fmt='%.1f', fontsize=8) # Show values on bars

        fig.tight_layout() # Adjust layout to prevent labels from overlapping
        plt.show()

# --- Reset controls function ---
def on_reset_button_clicked(b):
    global initial_slider_values, user_edited_qualitative_scores, current_scores_data, baseline_scores
    user_edited_qualitative_scores = {} # Clear user edits on reset
    current_scores_data = json.loads(json.dumps(baseline_scores)) # Reset scores to baseline

    revenue_slider.value = initial_slider_values['revenueAssumption']
    rd_spend_slider.value = initial_slider_values['rdSpendAssumption']
    market_comp_slider.value = initial_slider_values['marketCompLevel']
    reg_scrutiny_slider.value = initial_slider_values['regScrutinyLevel']
    llm_optimism_slider.value = initial_slider_values['llmOptimism']
    expert_criticality_slider.value = initial_slider_values['expertCriticality']

    # Reset qualitative score widgets to baseline
    for i, metric in enumerate(baseline_scores['qualitative']):
        qualitative_score_widgets[i]['llm_widget'].value = metric['llm']
        qualitative_score_widgets[i]['expert_widget'].value = metric['expert']

    recalculate_and_render_all()

reset_button.on_click(on_reset_button_clicked)

# --- Attach observers to sliders and company name ---
all_sliders = [revenue_slider, rd_spend_slider, market_comp_slider, reg_scrutiny_slider, llm_optimism_slider, expert_criticality_slider]
for slider in all_sliders:
    slider.observe(recalculate_and_render_all, names='value')
company_name_widget.observe(recalculate_and_render_all, names='value')


print("Core recalculation and rendering functions defined and observers attached.")
# ```

# Cell 16: Simulated "Expert Take" and "Counterarguments" (Markdown)
# ```markdown
# ## Simulated API Interactions
#
# These sections replicate the "Gemini button" functionality from the HTML, providing simulated analysis based on the current state of the interactive controls.
# ```

# Cell 17: Simulated API Interaction Widgets and Functions (Code)
# ```python
# --- Output areas for simulated API calls ---
expert_take_llm_output = widgets.Output()
counterarguments_expert_output = widgets.Output()

# --- Buttons ---
expert_take_llm_button = widgets.Button(description="âœ¨ Get Expert Take on LLM Snippets", button_style='info')
counterarguments_expert_button = widgets.Button(description="âœ¨ Suggest Counterarguments to Expert View", button_style='info')

# --- Simulated API call functions (translated from HTML JS) ---
def get_expert_take_on_llm(b=None):
    with expert_take_llm_output:
        clear_output(wait=True)
        display(widgets.HTML("<em>âœ¨ Fetching simulated expert perspective... (simulated delay)</em>"))
        # Simulate API delay (0.5-1s)
        # import time; time.sleep(random.uniform(0.5, 1.0))

        company = get_company_name()
        llm_optimism = llm_optimism_slider.value
        reg_scrutiny = reg_scrutiny_slider.value
        revenue = revenue_slider.value
        market_comp = market_comp_slider.value

        # Simulate getting the LLM snippets (text only for prompt)
        # This is a simplified version; in JS it parsed HTML. Here we use the global report_texts.
        # For a more accurate simulation, we'd re-generate/parse the text content of llm_snippets_display
        financial_snippet_text = ""
        risk_snippet_text = ""
        llm_sim_text = report_texts.get("llm_simulated", "Financial snippet unavailable. Risk snippet unavailable.")
        
        try:
            financial_part = llm_sim_text.split("Financial Overview (Revenue):")[1]
            financial_snippet_text = financial_part.split("Risk Factors (Regulatory):")[0].strip()[:200] + "..."
            risk_snippet_text = financial_part.split("Risk Factors (Regulatory):")[1].strip()[:200] + "..."
        except IndexError:
            financial_snippet_text = llm_sim_text[:200] + "..." # Fallback
            risk_snippet_text = "Risk snippet part not found in simulation."


        insights = f"<strong>Expert Analysis of LLM Snippets for {company}</strong> (LLM Optimism: {llm_optimism}/10, Revenue: ${revenue:.1f}B, Market Comp: {market_comp}/10, Reg. Scrutiny: {reg_scrutiny}/10):<br><br>"
        insights += f"<u>Regarding Financial Overview</u> (\"<em>{financial_snippet_text}</em>\"):<br>"
        insights += "<ul><li>An expert would seek quantification of \"adoption\" and \"uptake.\" What are the actual growth rates, market share, and customer acquisition costs?</li>"
        if llm_optimism > 7: insights += "<li>With high optimism, an expert would critically examine the sustainability of such growth. Are there dependencies or single points of failure?</li>"
        elif llm_optimism < 4: insights += "<li>If pessimistic, an expert would ask for specific reasons for headwinds and competitive pressures beyond general statements.</li>"
        insights += "<li>What is the margin profile of these API services and enterprise solutions? High revenue growth with poor margins is not sustainable.</li></ul>"

        insights += f"<br><u>Regarding Risk Factors</u> (\"<em>{risk_snippet_text}</em>\"):<br>"
        insights += "<ul><li>\"Important areas of focus\" is insufficient. An expert requires details on specific mitigation strategies, resource allocation, and KPIs for managing these risks.</li>"
        if reg_scrutiny > 7: insights += f"<li>Given high regulatory scrutiny ({reg_scrutiny}/10), the report should specify key regulations (e.g., EU AI Act, data sovereignty) and {company}'s precise compliance roadmap and potential financial impact of non-compliance.</li>"
        else: insights += f"<li>Even with moderate scrutiny ({reg_scrutiny}/10), an expert would probe for forward-looking risk assessments considering the rapid pace of AI development and potential for new regulations.</li>"
        insights += "<li>What is the C-level executive accountable for managing these specific regulatory and societal risks?</li></ul>"

        clear_output(wait=True)
        display(HTML(insights))

def get_counterarguments_to_expert(b=None):
    with counterarguments_expert_output:
        clear_output(wait=True)
        display(widgets.HTML("<em>âœ¨ Generating simulated counterarguments... (simulated delay)</em>"))
        # import time; time.sleep(random.uniform(0.5, 1.0))

        company = get_company_name()
        revenue = revenue_slider.value
        rd_spend = rd_spend_slider.value
        expert_criticality = expert_criticality_slider.value
        market_comp = market_comp_slider.value
        # reg_scrutiny = reg_scrutiny_slider.value # Not used in original JS counterarguments logic directly

        # expert_snippet_text = report_texts.get("expert_simulated", "Expert snippet unavailable")[:200] + "..." # For context if needed

        counter_args = f"<strong>Devil's Advocate - Counterarguments to Expert View for {company}</strong> (Expert Criticality: {expert_criticality}/10, Revenue: ${revenue:.1f}B, R&D: ${rd_spend:.1f}B):<br><br>"
        rd_to_revenue_ratio = (rd_spend / revenue) if revenue > 0 else float('inf')

        counter_args += "<ul>"
        if rd_to_revenue_ratio > 1.2 and expert_criticality > 6:
            counter_args += "<li>The expert's concern about high R&D spend might undervalue the strategic imperative in a winner-takes-most AI market. Securing foundational model leadership could justify near-term losses for immense long-term gains.</li>"
            counter_args += "<li>\"Cash burn\" could be strategically managed through long-term, low-cost capital partnerships (e.g., with major cloud providers) not fully visible in standard financial views.</li>"
        elif expert_criticality > 6:
            counter_args += f"<li>Focus on current COGS might overlook future economies of scale or proprietary hardware/software optimizations that could drastically improve margins as {company} scales its inference capabilities.</li>"

        counter_args += f"<li>The expert's assessment of \"sustainability\" might be based on traditional SaaS metrics. {company}'s model could be closer to deep-tech R&D or infrastructure play, where profitability horizons are longer and strategic partnerships are key.</li>"

        if market_comp < 5 and expert_criticality > 5: # Note: marketComp slider is 1=Low, 10=High
            counter_args += f"<li>If market competition ({market_comp}/10) is less intense than perceived by the expert, {company} might have more pricing power or a longer runway to optimize costs before facing severe margin pressure.</li>"
        else:
            counter_args += "<li>Unforeseen high-margin applications (e.g., specialized AI for drug discovery, autonomous systems) could emerge from current R&D, rapidly altering the financial outlook beyond current revenue streams.</li>"
        counter_args += f"<li>The expert might be underestimating the network effects and data flywheel advantages {company} is building, which could create a strong defensive moat against new entrants.</li></ul>"

        clear_output(wait=True)
        display(HTML(counter_args))

expert_take_llm_button.on_click(get_expert_take_on_llm)
counterarguments_expert_button.on_click(get_counterarguments_to_expert)

simulated_api_box = VBox([
    HBox([expert_take_llm_button, counterarguments_expert_button]),
    expert_take_llm_output,
    counterarguments_expert_output
])

print("Simulated API interaction setup complete.")
# ```

# Cell 18: Displaying the Main Interface and Initial Calculation (Code)
# ```python
# --- Main Display Arrangement ---
# This VBox will hold all visible parts of the dashboard

# Header for the main dashboard section
dashboard_header = widgets.HTML("<h2>ðŸ“Š Interactive Credit Report Analyzer Dashboard</h2>")

# Combine all display elements
# Order: Header, Controls, Report Snippets, User Inputs, Qualitative Editor, Table, Chart, Simulated APIs
dashboard_layout = VBox([
    dashboard_header,
    widgets.HTML("<hr><h3>0. Interactive Controls & Assumptions</h3>"),
    controls_box, # Sliders and reset button
    widgets.HTML("<hr><h3>III. Simulated Report Snippets & User Input</h3>"),
    reports_display_box, # Displays LLM and Expert simulated snippets
    report_input_box, # TextAreas for user to paste reports
    widgets.HTML("<hr><h3>V. Qualitative Score Editor</h3>"),
    qualitative_scores_editor_vbox, # Input boxes for qualitative scores
    widgets.HTML("<hr><h3>VI. Comparative Analysis Table & Chart</h3>"),
    comparison_table_output, # Output widget for the pandas table
    comparison_chart_output, # Output widget for the matplotlib chart
    widgets.HTML("<hr><h3>VII. Simulated Expert Commentary</h3>"),
    simulated_api_box # Buttons and output for "Gemini" calls
])

# Initial population of table, chart, and snippets
on_reset_button_clicked(None) # Call reset to initialize everything with defaults

# Display the entire dashboard
display(dashboard_layout)

print("Dashboard displayed. Initial calculations performed.")
# ```

# Cell 19: Textual Sections from HTML (Markdown)
# ```markdown
# ## I. Introduction: Setting the Stage
#
# (Content from your HTML Section I can be pasted here as Markdown)
#
# ### A. Project Rationale and Objectives
# The proliferation of Large Language Models (LLMs) into various domains, including finance, presents both opportunities and challenges...
# 1.  To simulate a credit report for the chosen company that mimics the characteristics of an LLM-generated output.
# 2.  To simulate a credit report for the chosen company that reflects the analytical depth and style of an expert credit analyst.
# 3.  To develop and implement a framework for generating a comparative analysis table, incorporating both qualitative and quantitative scoring, to evaluate the differences between these two simulated reports.
#
# ### B. The Significance of the Case Study
# The chosen company (e.g., OpenAI) serves as a compelling and complex subject for credit risk simulation...
#
# ### C. Overview of this Jupyter Notebook Workflow
# Jupyter Notebooks provide an ideal environment...
# * **Initial Setup:** Configuration, library imports, helper functions.
# * **Interactive Controls:** Sliders to adjust simulation parameters.
# * **User Input/Data Simulation:** Generating/Inputting LLM-style and expert-driven reports.
# * **Scoring Mechanism:** Simulated quantitative and qualitative scoring. Users can edit qualitative scores.
# * **Comparative Analysis:** Generating and displaying a comparison table and chart.
# * **Simulated Commentary:** "Expert take" and "counterarguments" based on simulated data.
#
# ---
# ## Further Sections (IV, V, VII, VIII, IX from HTML)
#
# (You can continue to translate the remaining textual sections from your HTML document into Markdown cells here to complete the narrative of the notebook. For example:)
#
# ### IV. Simulating an LLM-Generated Credit Report
# * **Conceptualizing LLM Output:** Fluency, potential for generic content, risk of hallucinations. The simulated snippets aim to reflect this.
# * **Python Techniques:** Template-based generation with stochasticity (as implemented in `generate_llm_snippets`).
#
# ### V. Simulating an Expert Credit Report
# * **Defining Characteristics:** Depth of analysis, specific financial metrics, nuanced risk assessment.
# * **Python Techniques:** Rule-based systems with embedded domain logic (as in `generate_expert_financial_analysis`).
#
# ### VIII. Advanced Considerations and Best Practices
# * **Data Persistence:** For actual reports, consider saving to/loading from files (e.g., `.txt`, `.json`).
# * **Error Handling:** Implement `try-except` blocks in more complex data processing.
# * **Actual NLP Analysis:** To score user-provided text, fully implement the NLP functions (sentiment, readability, keyword density using `TextBlob`, `textstat`, `spaCy`) and integrate their outputs into the `current_scores_data`. This notebook primarily uses *simulated* scores for the interactive table.
#
# ### IX. Conclusion and Future Directions
# This notebook establishes a methodology for interactively simulating, comparing, and discussing characteristics of LLM-generated vs. expert-analyzed reports.
# * **Limitations:** The simulation is stylized. Scoring is primarily illustrative unless full NLP is integrated for user inputs.
# * **Future Work:** Integrate real-time LLM API calls for report generation, expand NLP scoring capabilities, allow more granular control over simulation logic.
#
# ```
